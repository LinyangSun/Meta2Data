{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this code, make sure you:  \n",
    "\t1.\tGo to the NCBI website and select BioProject.  \n",
    "\t2.\tSearch for every combination of keywords youâ€™re interested in.  \n",
    "\t3.\tclick send to --> file --> formate(Accessions List) --> creat file, then download all the resulting files into a specific folder.  \n",
    "The code is for:  \n",
    "\t4.  code chunk 1 module installation  \n",
    "\t5.\tcode chunk 2 include functions for download data.  \n",
    "\t6.\tcode chunk 3 is the code for execute the code  \n",
    "Usage:  \n",
    "\t7.\tthe final file save to matched_sra_biosample_biosample_id.csv under output_folder.  \n",
    "\t8.  to use this code, repeat step 2-3 to get all bioproject id you want. save to input_folder. then define output folder  \n",
    "Noice:  \n",
    "  The code will take very long time for running. if you have a lots of Bioproject. If you want it execute on slurm, covert this code to .py and fit it with slurm system\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: biopython in /Users/a1-6/anaconda3/lib/python3.11/site-packages (1.83)\n",
      "Requirement already satisfied: pandas in /Users/a1-6/anaconda3/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in /Users/a1-6/anaconda3/lib/python3.11/site-packages (from biopython) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/a1-6/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/a1-6/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/a1-6/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/a1-6/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install biopython pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NCBI Data Download and Processing Pipeline\n",
      "======================================================================\n",
      "\n",
      "[Step 1/6] Reading BioProject IDs\n",
      "Found 4 unique BioProject IDs from 2 files\n",
      "  BioProjects: ['PRJNA1294982', 'PRJNA1310163', 'PRJNA1285920', 'PRJNA1253914']\n",
      "\n",
      "[Step 2/6] Downloading BioSample data for 4 BioProjects\n",
      "  [1/4] PRJNA1294982... 820 BioSamples\n",
      "  [2/4] PRJNA1310163... 18 BioSamples\n",
      "  [3/4] PRJNA1285920... 20 BioSamples\n",
      "  [4/4] PRJNA1253914... 98 BioSamples\n",
      "\n",
      "[Step 3/6] Parsing BioSample files\n",
      "  [1/4] PRJNA1253914_biosample.txt... 98 samples\n",
      "  [2/4] PRJNA1310163_biosample.txt... 18 samples\n",
      "  [3/4] PRJNA1285920_biosample.txt... 20 samples\n",
      "  [4/4] PRJNA1294982_biosample.txt... 820 samples\n",
      "  Combined: 956 samples, 34 columns\n",
      "  Saved to: /Users/a1-6/Desktop/GetMetaFromNCBI/TestData/DownloadedBioprojectID/meta/all_biosamples_combined.csv\n",
      "\n",
      "[Step 4/6] Downloading SRA data for 4 BioProjects\n",
      "  [1/4] PRJNA1294982... 820 runs\n",
      "  [2/4] PRJNA1310163... 18 runs\n",
      "  [3/4] PRJNA1285920... 20 runs\n",
      "  [4/4] PRJNA1253914... 98 runs\n",
      "\n",
      "[Step 5/6] Parsing SRA RunInfo files\n",
      "  [1/4] PRJNA1253914_sra_runinfo.csv... 98 runs\n",
      "  [2/4] PRJNA1294982_sra_runinfo.csv... 820 runs\n",
      "  [3/4] PRJNA1310163_sra_runinfo.csv... 18 runs\n",
      "  [4/4] PRJNA1285920_sra_runinfo.csv... 20 runs\n",
      "  Combined: 956 runs, 49 columns\n",
      "  Saved to: /Users/a1-6/Desktop/GetMetaFromNCBI/TestData/DownloadedBioprojectID/meta/all_sra_runs_combined.csv\n",
      "\n",
      "[Step 6/6] Merging SRA and BioSample data\n",
      "  BioSample records: 956\n",
      "  SRA records: 956\n",
      "  Common BioSample IDs: 956\n",
      "  Strategy 1 (BioSample ID): 956 matches\n",
      "  Strategy 2 (BioProject+BioSample): 956 matches\n",
      "  Using Strategy 1\n",
      "  Matched data saved: /Users/a1-6/Desktop/GetMetaFromNCBI/TestData/DownloadedBioprojectID/meta/matched_sra_biosample_biosample_id.csv (956 records)\n",
      "\n",
      "======================================================================\n",
      "Pipeline Complete - Summary\n",
      "======================================================================\n",
      "BioProjects: 4\n",
      "BioSamples: 956\n",
      "SRA Runs: 956\n",
      "Matched records: 956\n",
      "SRA-only records: 0\n",
      "BioSample-only records: 0\n",
      "\n",
      "All files saved to: /Users/a1-6/Desktop/GetMetaFromNCBI/TestData/DownloadedBioprojectID/meta\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NCBI Data Download and Parsing Pipeline\n",
    "Complete workflow for downloading and merging BioProject, BioSample, and SRA data\n",
    "\"\"\"\n",
    "\n",
    "from Bio import Entrez\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Core Functions\n",
    "# ============================================================================\n",
    "\n",
    "def read_bioproject_ids(folder_path):\n",
    "    \"\"\"Read BioProject IDs from all txt files in folder\"\"\"\n",
    "    txt_files = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "    \n",
    "    df_list = []\n",
    "    for file in txt_files:\n",
    "        temp_df = pd.read_csv(file, sep='\\t', engine='python', header=None, \n",
    "                              names=['BioProject_ID'])\n",
    "        temp_df['source_file'] = os.path.basename(file)\n",
    "        df_list.append(temp_df)\n",
    "    \n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    df = df.drop_duplicates(subset=['BioProject_ID'], keep='first')\n",
    "    \n",
    "    print(f\"Found {df.shape[0]} unique BioProject IDs from {len(txt_files)} files\")\n",
    "    return df\n",
    "\n",
    "def get_biosamples_from_bioproject(bioproject_id):\n",
    "    \"\"\"Fetch all BioSample IDs linked to a BioProject\"\"\"\n",
    "    try:\n",
    "        search_handle = Entrez.esearch(db=\"bioproject\", term=bioproject_id)\n",
    "        search_results = Entrez.read(search_handle)\n",
    "        search_handle.close()\n",
    "        \n",
    "        if not search_results[\"IdList\"]:\n",
    "            return []\n",
    "        \n",
    "        bioproject_uid = search_results[\"IdList\"][0]\n",
    "        \n",
    "        link_handle = Entrez.elink(dbfrom=\"bioproject\", db=\"biosample\", id=bioproject_uid)\n",
    "        link_results = Entrez.read(link_handle)\n",
    "        link_handle.close()\n",
    "        \n",
    "        biosample_ids = []\n",
    "        if link_results and link_results[0][\"LinkSetDb\"]:\n",
    "            for link in link_results[0][\"LinkSetDb\"][0][\"Link\"]:\n",
    "                biosample_ids.append(link[\"Id\"])\n",
    "        \n",
    "        return biosample_ids\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error fetching BioSamples: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def download_biosample_data(bioproject_id, biosample_ids, output_dir=\".\"):\n",
    "    \"\"\"Download BioSample metadata\"\"\"\n",
    "    try:\n",
    "        if not biosample_ids:\n",
    "            return None\n",
    "        \n",
    "        fetch_handle = Entrez.efetch(db=\"biosample\", id=\",\".join(biosample_ids),\n",
    "                                     rettype=\"full\", retmode=\"text\")\n",
    "        data = fetch_handle.read()\n",
    "        fetch_handle.close()\n",
    "        \n",
    "        output_file = os.path.join(output_dir, f\"{bioproject_id}_biosample.txt\")\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(data)\n",
    "        \n",
    "        return output_file\n",
    "    except Exception as e:\n",
    "        print(f\"  Error downloading BioSample: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def batch_download_biosamples(bioproject_list, output_dir=\".\"):\n",
    "    \"\"\"Batch download BioSample data for multiple BioProjects\"\"\"\n",
    "    print(f\"\\n[Step 2/6] Downloading BioSample data for {len(bioproject_list)} BioProjects\")\n",
    "    \n",
    "    results = []\n",
    "    for i, bioproject_id in enumerate(bioproject_list, 1):\n",
    "        print(f\"  [{i}/{len(bioproject_list)}] {bioproject_id}...\", end=\" \")\n",
    "        \n",
    "        biosample_ids = get_biosamples_from_bioproject(bioproject_id)\n",
    "        \n",
    "        result = {\n",
    "            'BioProject_ID': bioproject_id,\n",
    "            'BioSample_Count': len(biosample_ids),\n",
    "            'TXT_File': None\n",
    "        }\n",
    "        \n",
    "        if biosample_ids:\n",
    "            txt_file = download_biosample_data(bioproject_id, biosample_ids, output_dir)\n",
    "            result['TXT_File'] = txt_file\n",
    "            print(f\"{len(biosample_ids)} BioSamples\")\n",
    "        else:\n",
    "            print(\"No BioSamples found\")\n",
    "        \n",
    "        results.append(result)\n",
    "        time.sleep(0.4)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "class BioSampleParser:\n",
    "    \"\"\"Parser for BioSample text files\"\"\"\n",
    "    \n",
    "    def parse_file(self, file_path):\n",
    "        \"\"\"Parse BioSample file and extract sample information\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        sample_blocks = re.split(r'\\n(?=\\d+:\\s+)', content)\n",
    "        samples = []\n",
    "        \n",
    "        for block in sample_blocks:\n",
    "            if not block.strip():\n",
    "                continue\n",
    "            sample_data = self._parse_sample_block(block)\n",
    "            if sample_data:\n",
    "                samples.append(sample_data)\n",
    "        \n",
    "        return pd.DataFrame(samples)\n",
    "    \n",
    "    def _parse_sample_block(self, block):\n",
    "        \"\"\"Parse individual sample block\"\"\"\n",
    "        data = {}\n",
    "        lines = block.split('\\n')\n",
    "        \n",
    "        first_line = lines[0]\n",
    "        match = re.match(r'(\\d+):\\s*(.+)', first_line)\n",
    "        if match:\n",
    "            data['Sample_Number'] = int(match.group(1))\n",
    "            data['Sample_Name'] = match.group(2).strip()\n",
    "        \n",
    "        current_section = None\n",
    "        attributes_lines = []\n",
    "        description_text = []\n",
    "        \n",
    "        for line in lines[1:]:\n",
    "            line_stripped = line.strip()\n",
    "            \n",
    "            if line_stripped.startswith('Identifiers:'):\n",
    "                current_section = 'identifiers'\n",
    "                self._parse_identifiers(line_stripped.replace('Identifiers:', '').strip(), data)\n",
    "            elif line_stripped.startswith('Organism:'):\n",
    "                data['Organism'] = line_stripped.replace('Organism:', '').strip()\n",
    "            elif line_stripped.startswith('Attributes:'):\n",
    "                current_section = 'attributes'\n",
    "            elif line_stripped.startswith('Description:'):\n",
    "                current_section = 'description'\n",
    "            elif line_stripped.startswith('Keywords:'):\n",
    "                data['Keywords'] = line_stripped.replace('Keywords:', '').strip()\n",
    "            elif line_stripped.startswith('Accession:'):\n",
    "                parts = line_stripped.split('\\t')\n",
    "                for part in parts:\n",
    "                    if 'Accession:' in part:\n",
    "                        data['Accession'] = part.replace('Accession:', '').strip()\n",
    "                    elif 'ID:' in part:\n",
    "                        data['ID'] = part.replace('ID:', '').strip()\n",
    "            elif current_section == 'identifiers' and line_stripped:\n",
    "                self._parse_identifiers(line_stripped, data)\n",
    "            elif current_section == 'attributes' and line_stripped.startswith('/'):\n",
    "                attributes_lines.append(line_stripped)\n",
    "            elif current_section == 'description' and line_stripped:\n",
    "                description_text.append(line_stripped)\n",
    "        \n",
    "        if description_text:\n",
    "            data['Description'] = ' '.join(description_text)\n",
    "        \n",
    "        for attr_line in attributes_lines:\n",
    "            self._parse_attribute(attr_line, data)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def _parse_identifiers(self, text, data):\n",
    "        \"\"\"Parse identifier line\"\"\"\n",
    "        biosample_match = re.search(r'BioSample:\\s*(\\S+)', text)\n",
    "        if biosample_match:\n",
    "            data['BioSample'] = biosample_match.group(1).rstrip(';')\n",
    "        \n",
    "        sample_match = re.search(r'Sample name:\\s*([^;]+)', text)\n",
    "        if sample_match:\n",
    "            data['Sample_Name_Full'] = sample_match.group(1).strip()\n",
    "        \n",
    "        sra_match = re.search(r'SRA:\\s*(\\S+)', text)\n",
    "        if sra_match:\n",
    "            data['SRA'] = sra_match.group(1)\n",
    "    \n",
    "    def _parse_attribute(self, line, data):\n",
    "        \"\"\"Parse single attribute line\"\"\"\n",
    "        match = re.match(r'/([^=]+)=\"([^\"]*)\"', line)\n",
    "        if match:\n",
    "            key = match.group(1).strip()\n",
    "            value = match.group(2).strip()\n",
    "            clean_key = key.replace(' ', '_').replace('-', '_')\n",
    "            data[clean_key] = value\n",
    "\n",
    "def batch_parse_biosample_files(directory, output_file=\"all_biosamples_combined.csv\"):\n",
    "    \"\"\"Parse all BioSample txt files in directory\"\"\"\n",
    "    print(f\"\\n[Step 3/6] Parsing BioSample files\")\n",
    "    \n",
    "    directory = Path(directory)\n",
    "    txt_files = list(directory.glob(\"*_biosample.txt\"))\n",
    "    \n",
    "    if not txt_files:\n",
    "        print(\"  No BioSample files found\")\n",
    "        return None\n",
    "    \n",
    "    all_dataframes = []\n",
    "    parser = BioSampleParser()\n",
    "    \n",
    "    for i, file_path in enumerate(txt_files, 1):\n",
    "        print(f\"  [{i}/{len(txt_files)}] {file_path.name}...\", end=\" \")\n",
    "        try:\n",
    "            df = parser.parse_file(file_path)\n",
    "            df['Source_File'] = file_path.name\n",
    "            \n",
    "            bioproject_match = re.search(r'(PRJNA\\d+)', file_path.name)\n",
    "            if bioproject_match:\n",
    "                df['BioProject_ID'] = bioproject_match.group(1)\n",
    "            \n",
    "            all_dataframes.append(df)\n",
    "            print(f\"{len(df)} samples\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_dataframes:\n",
    "        return None\n",
    "    \n",
    "    df_combined = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    output_path = directory / output_file\n",
    "    df_combined.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"  Combined: {len(df_combined)} samples, {len(df_combined.columns)} columns\")\n",
    "    print(f\"  Saved to: {output_path}\")\n",
    "    \n",
    "    return df_combined\n",
    "\n",
    "def get_sra_runs_from_bioproject(bioproject_id):\n",
    "    \"\"\"Fetch all SRA Run IDs linked to a BioProject\"\"\"\n",
    "    try:\n",
    "        search_query = f\"{bioproject_id}[BioProject]\"\n",
    "        search_handle = Entrez.esearch(db=\"sra\", term=search_query, retmax=10000)\n",
    "        search_results = Entrez.read(search_handle)\n",
    "        search_handle.close()\n",
    "        \n",
    "        sra_ids = search_results[\"IdList\"]\n",
    "        return sra_ids\n",
    "    except Exception as e:\n",
    "        print(f\"  Error fetching SRA runs: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def download_sra_metadata(bioproject_id, sra_ids, output_dir=\".\"):\n",
    "    \"\"\"Download SRA RunInfo metadata\"\"\"\n",
    "    if not sra_ids:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        fetch_handle = Entrez.efetch(db=\"sra\", id=\",\".join(sra_ids),\n",
    "                                     rettype=\"runinfo\", retmode=\"text\")\n",
    "        txt_data = fetch_handle.read()\n",
    "        fetch_handle.close()\n",
    "        \n",
    "        txt_file = os.path.join(output_dir, f\"{bioproject_id}_sra_runinfo.csv\")\n",
    "        \n",
    "        if isinstance(txt_data, bytes):\n",
    "            with open(txt_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(txt_data.decode('utf-8'))\n",
    "        else:\n",
    "            with open(txt_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(txt_data)\n",
    "        \n",
    "        return txt_file\n",
    "    except Exception as e:\n",
    "        print(f\"  Error downloading SRA: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def batch_download_sra_data(bioproject_list, output_dir=\".\"):\n",
    "    \"\"\"Batch download SRA Run data for multiple BioProjects\"\"\"\n",
    "    print(f\"\\n[Step 4/6] Downloading SRA data for {len(bioproject_list)} BioProjects\")\n",
    "    \n",
    "    results = []\n",
    "    for i, bioproject_id in enumerate(bioproject_list, 1):\n",
    "        print(f\"  [{i}/{len(bioproject_list)}] {bioproject_id}...\", end=\" \")\n",
    "        \n",
    "        sra_ids = get_sra_runs_from_bioproject(bioproject_id)\n",
    "        \n",
    "        result = {\n",
    "            'BioProject_ID': bioproject_id,\n",
    "            'SRA_Run_Count': len(sra_ids),\n",
    "            'RunInfo_File': None\n",
    "        }\n",
    "        \n",
    "        if sra_ids:\n",
    "            txt_file = download_sra_metadata(bioproject_id, sra_ids, output_dir)\n",
    "            result['RunInfo_File'] = txt_file\n",
    "            print(f\"{len(sra_ids)} runs\")\n",
    "        else:\n",
    "            print(\"No SRA runs found\")\n",
    "        \n",
    "        results.append(result)\n",
    "        time.sleep(0.4)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def parse_sra_runinfo_files(directory, output_file=\"all_sra_runs_combined.csv\"):\n",
    "    \"\"\"Parse all SRA RunInfo CSV files\"\"\"\n",
    "    print(f\"\\n[Step 5/6] Parsing SRA RunInfo files\")\n",
    "    \n",
    "    directory = Path(directory)\n",
    "    csv_files = list(directory.glob(\"*_sra_runinfo.csv\"))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"  No SRA RunInfo files found\")\n",
    "        return None\n",
    "    \n",
    "    all_dfs = []\n",
    "    \n",
    "    for i, csv_file in enumerate(csv_files, 1):\n",
    "        print(f\"  [{i}/{len(csv_files)}] {csv_file.name}...\", end=\" \")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            df['Source_File'] = csv_file.name\n",
    "            \n",
    "            bioproject_match = re.search(r'(PRJNA\\d+)', csv_file.name)\n",
    "            if bioproject_match:\n",
    "                df['BioProject_ID_from_file'] = bioproject_match.group(1)\n",
    "            \n",
    "            all_dfs.append(df)\n",
    "            print(f\"{len(df)} runs\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_dfs:\n",
    "        return None\n",
    "    \n",
    "    df_combined = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    output_path = directory / output_file\n",
    "    df_combined.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"  Combined: {len(df_combined)} runs, {len(df_combined.columns)} columns\")\n",
    "    print(f\"  Saved to: {output_path}\")\n",
    "    \n",
    "    return df_combined\n",
    "\n",
    "def merge_sra_biosample_data(biosample_file, sra_file, output_dir):\n",
    "    \"\"\"Merge SRA and BioSample data using optimal strategy\"\"\"\n",
    "    print(f\"\\n[Step 6/6] Merging SRA and BioSample data\")\n",
    "    \n",
    "    # Load data\n",
    "    df_biosample = pd.read_csv(biosample_file)\n",
    "    df_sra = pd.read_csv(sra_file)\n",
    "    \n",
    "    print(f\"  BioSample records: {len(df_biosample)}\")\n",
    "    print(f\"  SRA records: {len(df_sra)}\")\n",
    "    \n",
    "    # Check overlap\n",
    "    biosample_ids_bio = set(df_biosample['BioSample'].dropna())\n",
    "    biosample_ids_sra = set(df_sra['BioSample'].dropna())\n",
    "    common_ids = biosample_ids_bio & biosample_ids_sra\n",
    "    \n",
    "    print(f\"  Common BioSample IDs: {len(common_ids)}\")\n",
    "    \n",
    "    # Strategy 1: Merge by BioSample ID only\n",
    "    df_merged_simple = df_sra.merge(\n",
    "        df_biosample,\n",
    "        left_on='BioSample',\n",
    "        right_on='BioSample',\n",
    "        how='outer',\n",
    "        suffixes=('_sra', '_biosample'),\n",
    "        indicator=True\n",
    "    )\n",
    "    \n",
    "    merge_stats = df_merged_simple['_merge'].value_counts()\n",
    "    matched_simple = merge_stats.get('both', 0)\n",
    "    \n",
    "    print(f\"  Strategy 1 (BioSample ID): {matched_simple} matches\")\n",
    "    \n",
    "    # Strategy 2: Merge by BioProject + BioSample\n",
    "    df_biosample_temp = df_biosample.copy()\n",
    "    df_sra_temp = df_sra.copy()\n",
    "    \n",
    "    df_biosample_temp['merge_key'] = (\n",
    "        df_biosample_temp['BioProject_ID'].astype(str) + '_' + \n",
    "        df_biosample_temp['BioSample'].astype(str)\n",
    "    )\n",
    "    df_sra_temp['merge_key'] = (\n",
    "        df_sra_temp['BioProject'].astype(str) + '_' + \n",
    "        df_sra_temp['BioSample'].astype(str)\n",
    "    )\n",
    "    \n",
    "    df_merged_combined = df_sra_temp.merge(\n",
    "        df_biosample_temp,\n",
    "        on='merge_key',\n",
    "        how='outer',\n",
    "        suffixes=('_sra', '_biosample'),\n",
    "        indicator=True\n",
    "    )\n",
    "    \n",
    "    merge_stats_combined = df_merged_combined['_merge'].value_counts()\n",
    "    matched_combined = merge_stats_combined.get('both', 0)\n",
    "    \n",
    "    print(f\"  Strategy 2 (BioProject+BioSample): {matched_combined} matches\")\n",
    "    \n",
    "    # Select best strategy\n",
    "    if matched_simple >= matched_combined:\n",
    "        print(f\"  Using Strategy 1\")\n",
    "        df_final = df_merged_simple\n",
    "        method = \"biosample_id\"\n",
    "    else:\n",
    "        print(f\"  Using Strategy 2\")\n",
    "        df_final = df_merged_combined\n",
    "        method = \"bioproject_biosample\"\n",
    "    \n",
    "    # Save results\n",
    "    output_dir = Path(output_dir)\n",
    "    \n",
    "    # Matched data\n",
    "    df_matched = df_final[df_final['_merge'] == 'both'].copy()\n",
    "    df_matched = df_matched.drop('_merge', axis=1)\n",
    "    matched_file = output_dir / f\"matched_sra_biosample_{method}.csv\"\n",
    "    df_matched.to_csv(matched_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"  Matched data saved: {matched_file} ({len(df_matched)} records)\")\n",
    "    \n",
    "    # Unmatched data\n",
    "    df_sra_only = df_final[df_final['_merge'] == 'left_only'].copy()\n",
    "    df_biosample_only = df_final[df_final['_merge'] == 'right_only'].copy()\n",
    "    \n",
    "    if len(df_sra_only) > 0:\n",
    "        sra_only_file = output_dir / f\"unmatched_sra_only_{method}.csv\"\n",
    "        df_sra_only.to_csv(sra_only_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"  SRA-only data saved: {sra_only_file} ({len(df_sra_only)} records)\")\n",
    "    \n",
    "    if len(df_biosample_only) > 0:\n",
    "        biosample_only_file = output_dir / f\"unmatched_biosample_only_{method}.csv\"\n",
    "        df_biosample_only.to_csv(biosample_only_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"  BioSample-only data saved: {biosample_only_file} ({len(df_biosample_only)} records)\")\n",
    "    \n",
    "    return {\n",
    "        'merged': df_final,\n",
    "        'matched': df_matched,\n",
    "        'sra_only': df_sra_only,\n",
    "        'biosample_only': df_biosample_only,\n",
    "        'method': method\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# Main Pipeline\n",
    "# ============================================================================\n",
    "\n",
    "def run_complete_pipeline(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Execute complete NCBI data download and processing pipeline\n",
    "    \n",
    "    Args:\n",
    "        input_folder: Path to folder containing BioProject ID txt files\n",
    "        output_folder: Path to output folder\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing all results\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"NCBI Data Download and Processing Pipeline\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Read BioProject IDs\n",
    "    print(\"\\n[Step 1/6] Reading BioProject IDs\")\n",
    "    df_bioprojects = read_bioproject_ids(input_folder)\n",
    "    bioproject_ids = df_bioprojects['BioProject_ID'].tolist()\n",
    "    print(f\"  BioProjects: {bioproject_ids}\")\n",
    "    \n",
    "    # Step 2-3: Download and parse BioSample data\n",
    "    biosample_download_results = batch_download_biosamples(bioproject_ids, output_folder)\n",
    "    df_biosamples = batch_parse_biosample_files(output_folder, \"all_biosamples_combined.csv\")\n",
    "    \n",
    "    # Step 4-5: Download and parse SRA data\n",
    "    sra_download_results = batch_download_sra_data(bioproject_ids, output_folder)\n",
    "    df_sra = parse_sra_runinfo_files(output_folder, \"all_sra_runs_combined.csv\")\n",
    "    \n",
    "    # Step 6: Merge data\n",
    "    biosample_file = os.path.join(output_folder, \"all_biosamples_combined.csv\")\n",
    "    sra_file = os.path.join(output_folder, \"all_sra_runs_combined.csv\")\n",
    "    \n",
    "    if os.path.exists(biosample_file) and os.path.exists(sra_file):\n",
    "        merge_results = merge_sra_biosample_data(biosample_file, sra_file, output_folder)\n",
    "        \n",
    "        # Final summary\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"Pipeline Complete - Summary\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"BioProjects: {len(bioproject_ids)}\")\n",
    "        print(f\"BioSamples: {len(df_biosamples) if df_biosamples is not None else 0}\")\n",
    "        print(f\"SRA Runs: {len(df_sra) if df_sra is not None else 0}\")\n",
    "        print(f\"Matched records: {len(merge_results['matched'])}\")\n",
    "        print(f\"SRA-only records: {len(merge_results['sra_only'])}\")\n",
    "        print(f\"BioSample-only records: {len(merge_results['biosample_only'])}\")\n",
    "        print(f\"\\nAll files saved to: {output_folder}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return {\n",
    "            'bioproject_ids': bioproject_ids,\n",
    "            'biosamples': df_biosamples,\n",
    "            'sra_runs': df_sra,\n",
    "            'merge_results': merge_results\n",
    "        }\n",
    "    else:\n",
    "        print(\"\\nWarning: Missing BioSample or SRA files, skipping merge\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Execution\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure paths\n",
    "    # Configure NCBI Entrez email\n",
    "    Entrez.email = \"....\"\n",
    "    input_folder = \"/Users/a1-6/Desktop/GetMetaFromNCBI/TestData/DownloadedBioprojectID/\"\n",
    "    output_folder = \"/Users/a1-6/Desktop/GetMetaFromNCBI/TestData/DownloadedBioprojectID/meta\"\n",
    "    os.makedirs(output_folder,exist_ok=True)\n",
    "    # Run pipeline\n",
    "    results = run_complete_pipeline(input_folder, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
