#!/usr/bin/env python3
"""
Unified Metadata Download Pipeline V4.2
Single-project processing with keyword search support

NEW IN V4.2:
- Keyword search mode: Search BioProjects by field, organism, optional terms
- Auto-generated email: No email required (auto-generated by default)
- Dual workflow: Keywords mode OR BioProject ID input mode

NEW IN V4.1:
- Status tracking: Generate status.tsv with download results
- Data validation: Skip BioProjects without Run information
- Process each BioProject individually before final merge
- Unified column patterns for both CNCB and NCBI data
- Native CNCB/GSA downloader (no iSeq dependency)
- Parallel downloads with ThreadPoolExecutor
- Checkpoint/resume system

Dependencies:
   - NCBI Entrez API (Biopython)
   - pandas, openpyxl, requests
"""

from Bio import Entrez
import time
import os
import csv
import pandas as pd
import re
import requests
from io import StringIO
from pathlib import Path
import glob
import argparse
import sys
import json
import threading
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import quote
from typing import Optional, Dict, List, Tuple
from itertools import product
import hashlib

# ============================================================================
# Configuration
# ============================================================================

DEFAULT_MAX_WORKERS = 3
DEFAULT_RETRY_ATTEMPTS = 5
DEFAULT_RETRY_DELAY = 1.5
DEFAULT_REQUEST_DELAY = 0.4

UNIFIED_PATTERNS = {
    'Run': re.compile(r'^[CEDS]RR\d+$'),
    'BioProject': re.compile(r'^PRJ[CEDN][A-Z]\d+$'),
    'BioSample': re.compile(r'^SAM[CEDN][A-Z]?\d+$'),
    'Experiment': re.compile(r'^[CEDS]RX\d+$'),
}

PRIORITY_COLUMNS = ['Run', 'BioProject', 'BioSample', 'Experiment']

STATUS_HAS_DATA = 'has_data'
STATUS_NO_DATA = 'no_data'
STATUS_NO_RUN = 'no_run_info'
STATUS_DOWNLOAD_ERROR = 'download_error'
STATUS_INVALID_FORMAT = 'invalid_format'

# Load column renaming dictionary
COLUMN_RENAME_DICT = None
COLUMN_RENAME_JSON_PATH = Path(__file__).parent.parent.parent / "docs" / "NCBI_Biosample.json"

def load_column_rename_dict():
    """Load column renaming dictionary from JSON file"""
    global COLUMN_RENAME_DICT
    if COLUMN_RENAME_DICT is None:
        try:
            with open(COLUMN_RENAME_JSON_PATH, 'r', encoding='utf-8') as f:
                COLUMN_RENAME_DICT = json.load(f)
            print(f"✓ Loaded {len(COLUMN_RENAME_DICT)} column rename rules")
        except Exception as e:
            print(f"Warning: Failed to load column rename dict: {e}")
            COLUMN_RENAME_DICT = {}
    return COLUMN_RENAME_DICT


# ============================================================================
# Helper Functions
# ============================================================================

def generate_fake_email():
    """Generate a deterministic fake email for NCBI API"""
    timestamp = str(int(time.time() / 86400))
    hash_val = hashlib.md5(timestamp.encode()).hexdigest()[:8]
    return f"meta2data_{hash_val}@research.example.com"


# ============================================================================
# BioProject Keyword Search (NCBI + CNCB)
# ============================================================================

class BioProjectDownloader:
    """BioProject downloader - supports NCBI and CNCB keyword search"""

    def __init__(self, email: str):
        Entrez.email = email
        self.cncb_base_url = "https://ngdc.cncb.ac.cn"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Referer": f"{self.cncb_base_url}/search/"
        }

    def _setup_directories(self, output_dir: Path) -> Dict[str, Path]:
        """Create directory structure"""
        output_dir = Path(output_dir)
        dirs = {
            'root': output_dir,
            'ncbi_search': output_dir / 'tmp' / 'ncbi_keywords_search',
            'cncb_search': output_dir / 'tmp' / 'cncb_keywords_search',
            'results': output_dir / 'searched_keywords'
        }
        for path in dirs.values():
            path.mkdir(parents=True, exist_ok=True)
        return dirs

    def generate_search_queries(self, field: List[str], organism: List[str],
                               opt: Optional[List[str]] = None) -> List[Tuple[str, Dict]]:
        """Generate search query combinations"""
        queries = []
        if opt:
            for f, o, op in product(field, organism, opt):
                queries.append((f'("{f}") AND ("{o}") AND ("{op}")',
                              {'field': f, 'organism': o, 'opt': op}))
        else:
            for f, o in product(field, organism):
                queries.append((f'("{f}") AND ("{o}")',
                              {'field': f, 'organism': o, 'opt': None}))
        return queries

    def search_and_download_batch(self, field: List[str], organism: List[str],
                                  opt: Optional[List[str]] = None,
                                  output_dir: Path = Path("./downloads"),
                                  databases: List[str] = ["ncbi", "cncb"],
                                  file_format: str = "tsv",
                                  delay: float = 1.0) -> Dict:
        """Batch search and download"""
        dirs = self._setup_directories(output_dir)
        queries = self.generate_search_queries(field, organism, opt)

        print(f"Queries: {len(queries)} | Databases: {', '.join(databases)}\n")

        all_results = {"queries": [], "dirs": dirs}

        for idx, (query_string, keywords) in enumerate(queries, 1):
            print(f"[{idx}/{len(queries)}] {query_string}")

            query_dirname = self._sanitize_filename(query_string)
            results = {}

            if "ncbi" in databases:
                ncbi_dir = dirs['ncbi_search'] / query_dirname
                ncbi_dir.mkdir(parents=True, exist_ok=True)
                results["ncbi"] = self._download_from_ncbi(query_string, ncbi_dir)

            if "cncb" in databases:
                cncb_dir = dirs['cncb_search'] / query_dirname
                cncb_dir.mkdir(parents=True, exist_ok=True)
                results["cncb"] = self._download_from_cncb(query_string, cncb_dir, file_format)

            all_results["queries"].append({
                "query": query_string,
                "keywords": keywords,
                "files": results
            })

            if idx < len(queries):
                time.sleep(delay)

        self._save_summary(all_results, dirs['results'])
        return all_results

    def _download_from_ncbi(self, query: str, output_dir: Path) -> Optional[Path]:
        """Download from NCBI"""
        try:
            search_handle = Entrez.esearch(db="bioproject", term=query, retmax=10000)
            search_results = Entrez.read(search_handle)
            search_handle.close()

            id_list = search_results["IdList"]
            if not id_list:
                print(f"  NCBI: 0")
                return None

            print(f"  NCBI: {len(id_list)}")

            output_file = output_dir / f"ncbi_{int(time.time())}.xml"
            fetch_handle = Entrez.efetch(db="bioproject", id=id_list,
                                        rettype="xml", retmode="xml")
            output_file.write_bytes(fetch_handle.read())
            fetch_handle.close()
            time.sleep(0.5)
            return output_file
        except Exception as e:
            print(f"  NCBI error: {e}")
            return None

    def _download_from_cncb(self, query: str, output_dir: Path,
                           file_format: str = "tsv") -> Optional[Path]:
        """Download from CNCB"""
        try:
            encoded_query = quote(query)
            api_url = (
                f"{self.cncb_base_url}/search/api/download/specific"
                f"?db=bioproject&q={encoded_query}&type={file_format}"
            )

            response = requests.get(api_url, headers=self.headers, timeout=120)
            response.raise_for_status()

            if len(response.content) < 100:
                print(f"  CNCB: 0 results")
                return None

            timestamp = int(time.time())
            output_file = output_dir / f"cncb_{timestamp}.{file_format}"
            output_file.write_bytes(response.content)

            try:
                df = pd.read_csv(output_file, sep='\t' if file_format == 'tsv' else ',')
                print(f"  CNCB: {len(df)} results")
            except:
                pass  # 即使读取失败也返回文件路径

            return output_file
        except Exception as e:
            print(f"  CNCB error: {e}")
            return None

    def _sanitize_filename(self, text: str, max_length: int = 80) -> str:
        """Clean filename"""
        for char in '<>:"/\\|?*':
            text = text.replace(char, '_')
        text = text.replace(' ', '_').replace('"', '').replace("'", '')
        while '__' in text:
            text = text.replace('__', '_')
        return text[:max_length].strip('_')

    def _save_summary(self, all_results: Dict, results_dir: Path):
        """Save summary"""
        summary_file = results_dir / "search_summary.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"Total queries: {len(all_results['queries'])}\n")
            f.write(f"Time: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            for idx, q in enumerate(all_results['queries'], 1):
                f.write(f"[{idx}] {q['query']}\n")
                for db, filepath in q['files'].items():
                    f.write(f"  {db.upper()}: {filepath.name if filepath else 'None'}\n")
        print(f"\n✓ Summary: {summary_file}\n")

    def parse_ncbi_xml(self, xml_file: Path) -> pd.DataFrame:
        """Parse NCBI XML"""
        from xml.etree import ElementTree as ET
        tree = ET.parse(xml_file)
        records = []

        for project in tree.getroot().findall('.//Project'):
            record = {}
            project_id = project.find('.//ProjectID/ArchiveID')
            if project_id is not None:
                record['accession'] = project_id.get('accession')

            title = project.find('.//ProjectDescr/Title')
            if title is not None:
                record['title'] = title.text

            description = project.find('.//ProjectDescr/Description')
            if description is not None:
                record['description'] = description.text

            organism = project.find('.//Organism/OrganismName')
            if organism is not None:
                record['organism'] = organism.text

            records.append(record)

        return pd.DataFrame(records)

    def combine_batch_results(self, all_results: Dict,
                             output_filename: str = "combined_results.csv") -> pd.DataFrame:
        """合并结果并清洗数据"""
        all_dfs = []

        print("Combining results...")

        for query_info in all_results['queries']:
            files = query_info.get('files', {})

            # NCBI
            if files.get('ncbi') and files['ncbi'].exists():
                try:
                    df = self.parse_ncbi_xml(files['ncbi'])
                    if not df.empty:
                        df['source'] = 'NCBI'
                        all_dfs.append(df)
                except:
                    pass

            # CNCB
            if files.get('cncb') and files['cncb'].exists():
                try:
                    cncb_file = files['cncb']
                    df = pd.read_csv(cncb_file,
                                   sep='\t' if cncb_file.suffix == '.tsv' else ',',
                                   encoding='utf-8',
                                   on_bad_lines='skip',
                                   engine='python')

                    if not df.empty:
                        columns_to_keep = {
                            'Accession': 'accession',
                            'Title': 'title',
                            'Description': 'description',
                            'Species': 'organism'
                        }

                        available_columns = {k: v for k, v in columns_to_keep.items()
                                           if k in df.columns}

                        df = df[list(available_columns.keys())].rename(columns=available_columns)
                        df['source'] = 'CNCB'

                        all_dfs.append(df)
                        print(f"  Parsed CNCB: {len(df)} records")
                except Exception as e:
                    print(f"  CNCB parse error: {e}")

        if not all_dfs:
            print("No data found")
            return pd.DataFrame()

        # 合并
        combined_df = pd.concat(all_dfs, ignore_index=True)
        initial_count = len(combined_df)

        # 清洗
        combined_df = combined_df.dropna(subset=['accession'])
        combined_df = combined_df[combined_df['accession'].str.startswith('PRJ', na=False)]
        combined_df = combined_df.drop_duplicates(subset=['accession'], keep='first')

        # 排序
        combined_df = combined_df[['accession', 'title', 'description', 'organism', 'source']]

        # 统计
        print(f"Initial: {initial_count} | Final: {len(combined_df)} unique records")
        print(f"NCBI: {len(combined_df[combined_df['source']=='NCBI'])} | "
              f"CNCB: {len(combined_df[combined_df['source']=='CNCB'])}")

        # 保存
        output_file = all_results['dirs']['results'] / output_filename
        combined_df.to_csv(output_file, index=False)
        print(f"✓ Saved: {output_file}\n")

        # 保存 BioProject IDs 到 txt 文件用于下游处理
        bioproject_ids_file = all_results['dirs']['results'] / "bioproject_ids.txt"
        combined_df['accession'].to_csv(bioproject_ids_file, index=False, header=False)
        print(f"✓ BioProject IDs saved: {bioproject_ids_file}\n")

        return combined_df


# ============================================================================
# State Management (Checkpoint System)
# ============================================================================

class StateManager:
    """Thread-safe state management for checkpoint/resume functionality"""

    def __init__(self, output_dir):
        self.output_dir = Path(output_dir)
        self.checkpoint_dir = self.output_dir / "checkpoints"
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)

        self.state_file = self.checkpoint_dir / "download_state.json"
        self.lock = threading.Lock()
        self.state = self._load_state()

    def _load_state(self):
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                pass

        return {
            'completed_projects': {},
            'failed_downloads': [],
            'status_records': {},
            'last_update': None
        }

    def save_state(self):
        with self.lock:
            self.state['last_update'] = datetime.now().isoformat()
            temp_file = self.state_file.with_suffix('.tmp')
            try:
                with open(temp_file, 'w', encoding='utf-8') as f:
                    json.dump(self.state, f, indent=2)
                temp_file.replace(self.state_file)
            except Exception:
                pass

    def is_completed(self, bioproject_id):
        csv_path = self.output_dir / f"{bioproject_id}.processed.csv"
        with self.lock:
            return bioproject_id in self.state['completed_projects'] and csv_path.exists()

    def mark_complete(self, bioproject_id, file_path, row_count, source):
        with self.lock:
            self.state['completed_projects'][bioproject_id] = {
                'file': str(file_path),
                'rows': row_count,
                'source': source,
                'timestamp': datetime.now().isoformat()
            }
            self.state['status_records'][bioproject_id] = STATUS_HAS_DATA
        self.save_state()

    def mark_failed(self, bioproject_id, error, stage):
        with self.lock:
            self.state['failed_downloads'].append({
                'bioproject_id': bioproject_id,
                'stage': stage,
                'error': str(error),
                'timestamp': datetime.now().isoformat()
            })
            if stage == 'validation':
                self.state['status_records'][bioproject_id] = STATUS_INVALID_FORMAT
            elif stage == 'no_run':
                self.state['status_records'][bioproject_id] = STATUS_NO_RUN
            elif stage == 'no_data':
                self.state['status_records'][bioproject_id] = STATUS_NO_DATA
            else:
                self.state['status_records'][bioproject_id] = STATUS_DOWNLOAD_ERROR
        self.save_state()

    def mark_status(self, bioproject_id, status):
        with self.lock:
            self.state['status_records'][bioproject_id] = status
        self.save_state()

    def get_stats(self):
        with self.lock:
            return {
                'completed': len(self.state['completed_projects']),
                'failed': len(self.state['failed_downloads'])
            }

    def get_all_status(self):
        with self.lock:
            return dict(self.state['status_records'])


# ============================================================================
# Retry Wrapper
# ============================================================================

def retry_wrapper(func, max_retries=DEFAULT_RETRY_ATTEMPTS, retry_delay=DEFAULT_RETRY_DELAY):
    """Retry wrapper with exponential backoff"""
    last_error = None
    for attempt in range(max_retries):
        try:
            return func()
        except Exception as e:
            last_error = e
            if attempt < max_retries - 1:
                time.sleep(retry_delay * (2 ** attempt))
    raise last_error


# ============================================================================
# BioProject Input Functions
# ============================================================================

def read_bioproject_ids(folder_path):
    """Read BioProject IDs from all txt files in folder"""
    txt_files = glob.glob(os.path.join(folder_path, "*.txt"))

    if not txt_files:
        print(f"ERROR: No txt files found in {folder_path}")
        return []

    all_ids = []
    for file in txt_files:
        try:
            with open(file, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        all_ids.append(line)
        except Exception as e:
            print(f"  Warning: Failed to read {file}: {e}")

    unique_ids = list(dict.fromkeys(all_ids))
    print(f"Found {len(unique_ids)} unique BioProject IDs from {len(txt_files)} files")

    return unique_ids


# ============================================================================
# Unified Column Cleaning Functions
# ============================================================================

def remove_empty_columns(df: pd.DataFrame) -> pd.DataFrame:
    """Remove columns that are empty"""
    non_empty_cols = []

    for col in df.columns:
        col_values = df[col].astype(str).str.strip()
        has_data = col_values.replace('', pd.NA).replace('nan', pd.NA).dropna().any()
        if has_data:
            non_empty_cols.append(col)

    return df[non_empty_cols]


def remove_duplicate_columns(df: pd.DataFrame) -> pd.DataFrame:
    """Remove duplicate columns"""
    cols_to_keep = []
    seen_values = {}

    for col in df.columns:
        col_values = tuple(df[col].astype(str).fillna('').tolist())
        if col_values not in seen_values:
            seen_values[col_values] = col
            cols_to_keep.append(col)

    return df[cols_to_keep]


def clean_and_standardize_columns(df: pd.DataFrame, source_prefix: str = None) -> pd.DataFrame:
    """Unified column cleaning for both CNCB and NCBI data"""
    if df.empty:
        return df

    df = remove_empty_columns(df)
    df = remove_duplicate_columns(df)

    new_columns = []
    cols_to_drop = []
    renamed = {}

    for col in df.columns:
        new_name = col
        new_name = re.sub(r'^(Sample|Experiment|Run)_', '', new_name)
        new_name = re.sub(r'(_sra|_biosample|_x|_y)$', '', new_name)

        if new_name == 'ID' and len(new_columns) == 0:
            cols_to_drop.append(col)
            continue

        col_values = df[col].astype(str).str.strip().unique()
        col_values = [v for v in col_values if v and v.lower() not in ['nan', 'none', '']]

        for standard_name, pattern in UNIFIED_PATTERNS.items():
            if col_values and all(pattern.match(str(v)) for v in col_values):
                if standard_name not in new_columns and standard_name not in renamed.values():
                    renamed[col] = standard_name
                    new_name = standard_name
                    break

        new_columns.append(new_name)

    if cols_to_drop:
        df = df.drop(columns=cols_to_drop)

    df.columns = new_columns

    first_cols = [c for c in PRIORITY_COLUMNS if c in df.columns]
    other_cols = [c for c in df.columns if c not in PRIORITY_COLUMNS]
    df = df[first_cols + other_cols]

    return df


def apply_column_rename_from_dict(df: pd.DataFrame) -> pd.DataFrame:
    """
    Apply column renaming based on NCBI_Biosample.json dictionary

    Dictionary format:
    {
        "Standard Name": ["variant1", "variant2", "variant3", ...]
    }
    """
    rename_dict = load_column_rename_dict()
    if not rename_dict:
        return df

    # Build reverse mapping: variant -> standard name
    variant_to_standard = {}
    for standard_name, variants in rename_dict.items():
        for variant in variants:
            variant_lower = variant.lower().strip()
            variant_to_standard[variant_lower] = standard_name

    # Apply renaming
    rename_map = {}
    for col in df.columns:
        col_lower = col.lower().strip()
        if col_lower in variant_to_standard:
            standard_name = variant_to_standard[col_lower]
            if standard_name not in df.columns:  # Avoid duplicate column names
                rename_map[col] = standard_name

    if rename_map:
        df = df.rename(columns=rename_map)
        print(f"  ✓ Renamed {len(rename_map)} columns using dictionary")

    return df


def normalize_column_to_camelcase(col_name: str) -> str:
    """
    Normalize column name to CamelCase format

    Examples:
        'library_strategy' -> 'LibraryStrategy'
        'Library_Strategy' -> 'LibraryStrategy'
        'sample type' -> 'SampleType'
        'host age' -> 'HostAge'
        'dna_extracted_a260a280' -> 'DnaExtractedA260a280'

    Args:
        col_name: Original column name

    Returns:
        CamelCase formatted column name without spaces or underscores
    """
    import re

    # Split by spaces, underscores, or hyphens
    parts = re.split(r'[\s_-]+', col_name.strip())

    # Capitalize first letter of each part
    camel_parts = [part.capitalize() for part in parts if part]

    # Join without separators
    return ''.join(camel_parts)


def apply_camelcase_normalization(df: pd.DataFrame) -> pd.DataFrame:
    """
    Normalize all column names to CamelCase format to reduce redundancy

    This converts columns like 'library_strategy', 'Library_Strategy', and
    'library strategy' to the same format 'LibraryStrategy', allowing them
    to merge naturally during final concatenation.

    If a normalized column name already exists (e.g., both 'LibraryStrategy' and
    'library_strategy'), this function will intelligently merge them:

    1. Compare values row by row (case-insensitive)
    2. If values are identical (ignoring case): keep CamelCase column value
    3. If values differ: concatenate with underscore (camel_value + "_" + source_value)
    4. If one is null/empty: use the non-null value
    5. Drop the redundant underscore/space version column

    Args:
        df: DataFrame with potentially inconsistent column names

    Returns:
        DataFrame with normalized CamelCase column names
    """
    rename_map = {}
    merge_needed = {}  # Track columns that need merging

    for col in df.columns:
        # Check if column contains spaces, underscores, or hyphens
        if ' ' in col or '_' in col or '-' in col:
            normalized = normalize_column_to_camelcase(col)

            # Only process if it creates a different name
            if normalized != col:
                if normalized in df.columns:
                    # Target column already exists - mark for merging
                    merge_needed[col] = normalized
                else:
                    # Safe to rename
                    rename_map[col] = normalized

    # First, intelligently merge duplicate columns
    if merge_needed:
        for source_col, target_col in merge_needed.items():
            # Create merged values row by row
            merged_values = []

            for idx in df.index:
                camel_val = df.loc[idx, target_col]
                source_val = df.loc[idx, source_col]

                # Convert to string for comparison, handle NaN
                camel_str = str(camel_val).strip() if pd.notna(camel_val) else ""
                source_str = str(source_val).strip() if pd.notna(source_val) else ""

                # Case 1: Both empty
                if not camel_str and not source_str:
                    merged_values.append(None)

                # Case 2: Only camel has value
                elif camel_str and not source_str:
                    merged_values.append(camel_val)

                # Case 3: Only source has value
                elif not camel_str and source_str:
                    merged_values.append(source_val)

                # Case 4: Both have values - compare (case-insensitive)
                else:
                    if camel_str.lower() == source_str.lower():
                        # Same value (ignoring case) - keep camel version
                        merged_values.append(camel_val)
                    else:
                        # Different values - concatenate with underscore
                        merged_values.append(f"{camel_val}_{source_val}")

            # Update the target column with merged values
            df[target_col] = merged_values

            # Drop the redundant source column
            df = df.drop(columns=[source_col])

        print(f"  ✓ Merged {len(merge_needed)} duplicate columns with intelligent value comparison")

    # Then, rename remaining columns
    if rename_map:
        df = df.rename(columns=rename_map)
        print(f"  ✓ Normalized {len(rename_map)} columns to CamelCase")

    return df


def validate_run_data(df: pd.DataFrame) -> bool:
    """Validate that DataFrame has Run column with valid data"""
    if df.empty:
        return False

    if 'Run' not in df.columns:
        return False

    run_pattern = UNIFIED_PATTERNS['Run']
    valid_runs = df['Run'].astype(str).apply(lambda x: bool(run_pattern.match(x)))
    valid_count = valid_runs.sum()

    if valid_count == 0:
        return False

    return True


# ============================================================================
# CNCB/GSA Download Functions
# ============================================================================

def download_cncb_metadata(accession: str, output_dir: Path) -> pd.DataFrame | None:
    """Download and process CNCB/GSA metadata for a BioProject"""
    BASE_URL = "https://ngdc.cncb.ac.cn/gsa"
    HEADERS = {"User-Agent": "Mozilla/5.0"}

    url = f"{BASE_URL}/search/getRunInfo"
    data = f'searchTerm=%26quot%3B{accession}%26quot%3BtotalDatas=9999%3BdownLoadCount=9999'

    try:
        resp = requests.post(url, data=data,
                           headers={**HEADERS, "Content-Type": "application/x-www-form-urlencoded"},
                           timeout=60)
        resp.raise_for_status()
        csv_content = resp.text
    except requests.RequestException:
        return None

    if csv_content.count('\n') < 2:
        return None

    temp_csv_path = output_dir / f"{accession}.temp.csv"
    temp_csv_path.write_text(csv_content, encoding='utf-8')

    # Step 1: 读取 CSV 作为主要数据源
    try:
        csv_df = pd.read_csv(StringIO(csv_content))
        if csv_df.empty:
            return None
        print(f"  ✓ CNCB CSV: {len(csv_df)} rows, {len(csv_df.columns)} columns")
    except Exception as e:
        print(f"  ✗ Failed to parse CSV: {e}")
        return None

    # Step 2: 尝试获取所有 CRA 的 Excel 数据
    cra_ids = set()
    for row in csv.DictReader(StringIO(csv_content)):
        submission = row.get('Submission', '').strip()
        if submission.startswith('CRA'):
            cra_ids.add(submission)
    cra_ids = sorted(cra_ids)

    all_excel_dfs = []  # 存储所有 Excel 数据

    if cra_ids:
        print(f"  ℹ Found {len(cra_ids)} CRA ID(s): {', '.join(cra_ids)}")

        for cra in cra_ids:
            try:
                resp = requests.post(f"{BASE_URL}/file/exportExcelFile",
                                   data={"type": "3", "dlAcession": cra},
                                   headers=HEADERS,
                                   timeout=60)
                resp.raise_for_status()

                temp_xlsx = output_dir / f"{cra}.temp.xlsx"
                temp_xlsx.write_bytes(resp.content)

                xlsx = pd.ExcelFile(temp_xlsx)
                sheet_dfs = []

                # 读取所有 sheets 并横向拼接
                for sheet in xlsx.sheet_names:
                    sheet_df = pd.read_excel(xlsx, sheet_name=sheet)
                    sheet_df.columns = [f"{sheet}_{col}" for col in sheet_df.columns]
                    sheet_dfs.append(sheet_df)

                if sheet_dfs:
                    # 横向拼接同一个 CRA 的所有 sheets
                    cra_merged = pd.concat(sheet_dfs, axis=1)
                    all_excel_dfs.append(cra_merged)
                    print(f"    ✓ {cra}: {len(cra_merged)} rows, {len(cra_merged.columns)} columns")

                temp_xlsx.unlink()

            except Exception as e:
                print(f"    ✗ {cra} failed: {e}")

    # Step 3: 合并所有 Excel 数据（如果有多个 CRA，按行拼接）
    if all_excel_dfs:
        if len(all_excel_dfs) == 1:
            excel_combined = all_excel_dfs[0]
        else:
            # 多个 CRA Excel 数据按行拼接 (axis=0)
            excel_combined = pd.concat(all_excel_dfs, axis=0, ignore_index=True)
            print(f"  ✓ Combined {len(all_excel_dfs)} Excel files: {len(excel_combined)} rows")
    else:
        excel_combined = None

    # Step 4: 合并 CSV 和 Excel（按列拼接 axis=1）
    if excel_combined is not None:
        # 确保行数一致才能横向合并
        if len(csv_df) == len(excel_combined):
            final_df = pd.concat([csv_df, excel_combined], axis=1)
            print(f"  ✓ Merged CSV + Excel: {len(final_df)} rows, {len(final_df.columns)} columns")
        else:
            print(f"  ⚠ Row count mismatch (CSV: {len(csv_df)}, Excel: {len(excel_combined)})")
            print(f"    Using CSV only")
            final_df = csv_df
    else:
        # 如果没有 Excel 数据，只使用 CSV
        print(f"  ℹ Using CSV data only (no Excel available)")
        final_df = csv_df

    # Apply column standardization
    final_df = clean_and_standardize_columns(final_df)

    # Apply dictionary-based column renaming
    final_df = apply_column_rename_from_dict(final_df)

    # Normalize column names to CamelCase to reduce redundancy
    final_df = apply_camelcase_normalization(final_df)

    return final_df


# ============================================================================
# NCBI Download Functions
# ============================================================================

def get_biosamples_from_bioproject(bioproject_id, email, api_key=None):
    """Fetch all BioSample IDs linked to a BioProject"""
    Entrez.email = email
    if api_key:
        Entrez.api_key = api_key

    def _fetch():
        search_handle = Entrez.esearch(db="bioproject", term=bioproject_id, retmax=1)
        search_results = Entrez.read(search_handle)
        search_handle.close()

        if not search_results["IdList"]:
            return []

        bioproject_uid = search_results["IdList"][0]

        link_handle = Entrez.elink(
            dbfrom="bioproject",
            db="biosample",
            id=bioproject_uid,
            retmax=10000
        )
        link_results = Entrez.read(link_handle)
        link_handle.close()

        biosample_ids = []
        if link_results and link_results[0].get("LinkSetDb"):
            for link in link_results[0]["LinkSetDb"][0]["Link"]:
                biosample_ids.append(link["Id"])

        return biosample_ids

    try:
        return retry_wrapper(_fetch)
    except Exception:
        return []


def download_biosample_data(bioproject_id, biosample_ids, output_dir, email, api_key=None):
    """Download BioSample metadata"""
    if not biosample_ids:
        return None

    Entrez.email = email
    if api_key:
        Entrez.api_key = api_key

    def _fetch():
        batch_size = 500
        all_data = []

        for i in range(0, len(biosample_ids), batch_size):
            batch = biosample_ids[i:i+batch_size]
            fetch_handle = Entrez.efetch(
                db="biosample",
                id=",".join(batch),
                rettype="full",
                retmode="text"
            )
            data = fetch_handle.read()
            fetch_handle.close()
            all_data.append(data)

            if len(biosample_ids) > batch_size:
                time.sleep(DEFAULT_REQUEST_DELAY)

        combined_data = "\n".join(all_data)
        output_file = output_dir / f"{bioproject_id}_biosample.txt"
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(combined_data)

        return output_file

    try:
        return retry_wrapper(_fetch)
    except Exception:
        return None


def download_sra_runinfo(bioproject_id, output_dir, email, api_key=None):
    """Download SRA RunInfo"""
    Entrez.email = email
    if api_key:
        Entrez.api_key = api_key

    def _fetch():
        search_handle = Entrez.esearch(db="sra", term=f"{bioproject_id}[BioProject]", retmax=10000)
        search_results = Entrez.read(search_handle)
        search_handle.close()

        if not search_results["IdList"]:
            return None

        sra_ids = search_results["IdList"]

        batch_size = 500
        all_data = []

        for i in range(0, len(sra_ids), batch_size):
            batch = sra_ids[i:i+batch_size]
            fetch_handle = Entrez.efetch(
                db="sra",
                id=",".join(batch),
                rettype="runinfo",
                retmode="text"
            )
            data = fetch_handle.read()
            fetch_handle.close()

            if isinstance(data, bytes):
                data = data.decode('utf-8')

            if i == 0:
                all_data.append(data)
            else:
                lines = data.split('\n')
                all_data.append('\n'.join(lines[1:]))

            if len(sra_ids) > batch_size:
                time.sleep(DEFAULT_REQUEST_DELAY)

        combined_data = '\n'.join(all_data)
        output_file = output_dir / f"{bioproject_id}_sra_runinfo.csv"
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(combined_data)

        return output_file

    try:
        return retry_wrapper(_fetch)
    except Exception:
        return None


# ============================================================================
# BioSample Parser
# ============================================================================

class BioSampleParser:
    """Parser for BioSample text files"""

    def __init__(self):
        self.samples = []

    def parse_file(self, file_path):
        """Parse BioSample text file"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        sample_blocks = re.split(r'\n(?=\d+:\s+)', content)

        for block in sample_blocks:
            if not block.strip():
                continue
            sample_data = self._parse_sample_block(block)
            if sample_data:
                self.samples.append(sample_data)

        return pd.DataFrame(self.samples)

    def _parse_sample_block(self, block):
        """Parse single BioSample block"""
        data = {}

        biosample_match = re.search(r'Accession:\s+(\S+)', block)
        if biosample_match:
            data['BioSample'] = biosample_match.group(1)

        patterns = {
            'Sample_Name': r'Identifiers:.*?Label:\s+(\S+)',
            'Organism': r'Organism:\s+(.+?)(?:\n|$)',
        }

        for key, pattern in patterns.items():
            match = re.search(pattern, block)
            if match:
                data[key] = match.group(1).strip()

        attr_section = re.search(r'Attributes:(.+?)(?=\n\n|\Z)', block, re.DOTALL)
        if attr_section:
            attr_text = attr_section.group(1)
            attr_matches = re.findall(r'/([^=]+?)="([^"]*)"', attr_text)
            for key, value in attr_matches:
                clean_key = key.strip().replace(' ', '_').replace('-', '_')
                data[clean_key] = value

        return data if data else None


def parse_biosample_file(file_path):
    """Parse BioSample file and return DataFrame"""
    if file_path is None or not Path(file_path).exists():
        return pd.DataFrame()

    try:
        parser = BioSampleParser()
        df = parser.parse_file(file_path)
        return df
    except Exception:
        return pd.DataFrame()


# ============================================================================
# NCBI Data Merge
# ============================================================================

def merge_ncbi_data_single(biosample_df, sra_df):
    """Merge BioSample and SRA data for a single BioProject"""
    if biosample_df.empty and sra_df.empty:
        return pd.DataFrame()

    if biosample_df.empty:
        return sra_df

    if sra_df.empty:
        return biosample_df

    if 'BioSample' in sra_df.columns and 'BioSample' in biosample_df.columns:
        merged_df = sra_df.merge(
            biosample_df,
            on='BioSample',
            how='outer',
            suffixes=('', '_biosample')
        )
        return merged_df

    return pd.concat([sra_df, biosample_df], axis=1)


def download_ncbi_metadata(bioproject_id: str, output_dir: Path, email: str,
                          api_key: str = None) -> pd.DataFrame | None:
    """Download and process NCBI metadata for a BioProject"""
    biosample_df = pd.DataFrame()
    sra_df = pd.DataFrame()

    try:
        biosample_ids = get_biosamples_from_bioproject(bioproject_id, email, api_key)
        if biosample_ids:
            biosample_file = download_biosample_data(bioproject_id, biosample_ids, output_dir, email, api_key)
            if biosample_file:
                biosample_df = parse_biosample_file(biosample_file)
    except Exception:
        pass

    time.sleep(DEFAULT_REQUEST_DELAY)

    try:
        sra_file = download_sra_runinfo(bioproject_id, output_dir, email, api_key)
        if sra_file:
            sra_df = pd.read_csv(sra_file)
    except Exception:
        pass

    if biosample_df.empty and sra_df.empty:
        return None

    merged_df = merge_ncbi_data_single(biosample_df, sra_df)

    # Apply column standardization
    merged_df = clean_and_standardize_columns(merged_df)

    # Apply dictionary-based column renaming
    merged_df = apply_column_rename_from_dict(merged_df)

    # Normalize column names to CamelCase to reduce redundancy
    merged_df = apply_camelcase_normalization(merged_df)

    return merged_df


# ============================================================================
# Single BioProject Processing
# ============================================================================

def process_single_bioproject(bioproject_id: str, output_dir: Path, email: str,
                             api_key: str, state_manager: StateManager) -> dict | None:
    """Process a single BioProject"""
    print(f"\n{'─'*50}")
    print(f"Processing: {bioproject_id}")
    print('─'*50)

    if state_manager.is_completed(bioproject_id):
        print(f"  ✓ Already processed (checkpoint)")
        csv_path = output_dir / f"{bioproject_id}.processed.csv"
        try:
            df = pd.read_csv(csv_path)
            return {'bioproject_id': bioproject_id, 'df': df, 'source': 'checkpoint'}
        except Exception:
            pass

    df = None
    source = None

    if re.match(r'^PRJC[A-Z]\d+$', bioproject_id):
        source = 'CNCB'
        df = download_cncb_metadata(bioproject_id, output_dir)

    elif re.match(r'^PRJ[EDN][A-Z]\d+$', bioproject_id):
        source = 'NCBI'
        df = download_ncbi_metadata(bioproject_id, output_dir, email, api_key)

    else:
        print(f"  Unknown BioProject format: {bioproject_id}")
        state_manager.mark_failed(bioproject_id, "Unknown format", 'validation')
        return None

    if df is None or df.empty:
        print(f"  ✗ No data retrieved")
        state_manager.mark_failed(bioproject_id, "No data", 'no_data')
        return None

    if not validate_run_data(df):
        print(f"  ✗ No valid Run information")
        state_manager.mark_failed(bioproject_id, "No Run info", 'no_run')
        return None

    df['Source_Database'] = source

    csv_path = output_dir / f"{bioproject_id}.processed.csv"
    df.to_csv(csv_path, index=False, encoding='utf-8')

    state_manager.mark_complete(bioproject_id, csv_path, len(df), source)

    print(f"  ✓ Saved: {csv_path.name}")

    return {'bioproject_id': bioproject_id, 'df': df, 'source': source}


# ============================================================================
# Parallel Processing
# ============================================================================

def parallel_process_bioprojects(bioproject_list, output_dir, email, api_key,
                                max_workers, state_manager):
    """Process multiple BioProjects in parallel"""
    print(f"\n{'='*70}")
    print(f"Processing {len(bioproject_list)} BioProjects")
    print(f"Workers: {max_workers}")
    print('='*70)

    all_results = []

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_bioproject = {
            executor.submit(
                process_single_bioproject,
                bioproject_id,
                output_dir,
                email,
                api_key,
                state_manager
            ): bioproject_id
            for bioproject_id in bioproject_list
        }

        for future in as_completed(future_to_bioproject):
            bioproject_id = future_to_bioproject[future]
            try:
                result = future.result()
                if result and result['df'] is not None:
                    all_results.append(result)
            except Exception as e:
                print(f"Task failed for {bioproject_id}: {e}")
                state_manager.mark_failed(bioproject_id, str(e), 'processing')

    print(f"\n{'='*70}")
    print(f"Processing Complete: {len(all_results)} / {len(bioproject_list)} successful")
    print('='*70)

    return all_results


# ============================================================================
# Status Report Generation
# ============================================================================

def generate_status_report(bioproject_ids, state_manager, output_dir):
    """Generate status.tsv file with download results"""
    status_file = output_dir / "status.tsv"

    status_records = state_manager.get_all_status()

    rows = []
    for bioproject_id in bioproject_ids:
        status = status_records.get(bioproject_id, 'unknown')
        rows.append({'BioProject': bioproject_id, 'Status': status})

    status_df = pd.DataFrame(rows)
    status_df.to_csv(status_file, sep='\t', index=False, encoding='utf-8')

    status_counts = status_df['Status'].value_counts().to_dict()

    print(f"\n{'='*70}")
    print("Status Report Generated")
    print('='*70)
    print(f"  File: {status_file}")
    print(f"  Total: {len(rows)} BioProjects")
    for status, count in status_counts.items():
        print(f"    - {status}: {count}")
    print('='*70)

    return status_df


# ============================================================================
# Final Merge
# ============================================================================

def merge_all_results(results, output_dir):
    """Merge all processed DataFrames into final output (按行合并)

    NOTE: This function reads ALL .processed.csv files from disk instead of
    relying on the results list, which may only contain newly processed items
    due to checkpoint/resume functionality.
    """
    print(f"\n{'='*70}")
    print("Final Merge")
    print('='*70)

    output_path = Path(output_dir)

    # Read ALL .processed.csv files from disk (not just from results list)
    processed_files = sorted(output_path.glob('*.processed.csv'))

    if not processed_files:
        print("  No .processed.csv files found in output directory")
        return pd.DataFrame()

    print(f"  Found {len(processed_files)} .processed.csv files")

    all_dfs = []
    source_counts = {}

    for csv_file in processed_files:
        try:
            df = pd.read_csv(csv_file)
            if not df.empty and 'Run' in df.columns:
                all_dfs.append(df)

                # Track source
                source = df['Source_Database'].iloc[0] if 'Source_Database' in df.columns else 'unknown'
                source_counts[source] = source_counts.get(source, 0) + len(df)

        except Exception as e:
            print(f"  ✗ Failed to read {csv_file.name}: {e}")

    if not all_dfs:
        print("  No valid DataFrames to merge")
        return pd.DataFrame()

    print(f"  Source breakdown: {source_counts}")
    print(f"  Total DataFrames to merge: {len(all_dfs)}")

    # 按行合并 (axis=0) - 确保所有数据按行拼接
    final_df = pd.concat(all_dfs, axis=0, ignore_index=True, sort=False)

    print(f"  After concat: {len(final_df)} rows, {len(final_df.columns)} columns")

    # Reorder columns: priority columns first
    first_cols = [c for c in PRIORITY_COLUMNS if c in final_df.columns]
    other_cols = [c for c in final_df.columns if c not in PRIORITY_COLUMNS]
    final_df = final_df[first_cols + other_cols]

    final_file = output_dir / "all_metadata_merged.csv"
    final_df.to_csv(final_file, index=False, encoding='utf-8-sig')

    print(f"\n{'='*70}")
    print(f"Final Merge Complete:")
    print(f"  Total records: {len(final_df)}")
    print(f"  Total columns: {len(final_df.columns)}")
    print(f"  Output: {final_file}")
    print('='*70)

    return final_df


# ============================================================================
# Main Pipeline V4.2
# ============================================================================

def run_unified_pipeline_v4(input_folder, output_folder, email, api_key=None, max_workers=None):
    """Execute unified metadata download pipeline V4.2"""
    output_path = Path(output_folder)
    output_path.mkdir(parents=True, exist_ok=True)

    state_manager = StateManager(output_folder)

    if max_workers is None:
        max_workers = 8 if api_key else DEFAULT_MAX_WORKERS

    print("\n" + "="*70)
    print("UNIFIED METADATA DOWNLOAD PIPELINE V4.2")
    print("="*70)
    print(f"Input:       {input_folder}")
    print(f"Output:      {output_folder}")
    print(f"Email:       {email}")
    print(f"API Key:     {'Yes' if api_key else 'No'}")
    print(f"Max Workers: {max_workers}")
    print("="*70)

    # Load column rename dictionary
    load_column_rename_dict()

    print("\n[Step 1] Reading BioProject IDs...")
    bioproject_ids = read_bioproject_ids(input_folder)

    if not bioproject_ids:
        print("ERROR: No BioProject IDs found")
        return None

    cncb_ids = [p for p in bioproject_ids if re.match(r'^PRJC[A-Z]\d+$', p)]
    ncbi_ids = [p for p in bioproject_ids if re.match(r'^PRJ[EDN][A-Z]\d+$', p)]
    other_ids = [p for p in bioproject_ids if p not in cncb_ids and p not in ncbi_ids]

    print(f"\nTotal BioProjects: {len(bioproject_ids)}")
    print(f"  CNCB (PRJC*): {len(cncb_ids)}")
    print(f"  NCBI (PRJ[EDN]*): {len(ncbi_ids)}")
    if other_ids:
        print(f"  Unknown format: {len(other_ids)}")
        for oid in other_ids:
            state_manager.mark_status(oid, STATUS_INVALID_FORMAT)

    stats = state_manager.get_stats()
    print(f"\nCheckpoint status:")
    print(f"  Completed: {stats['completed']}")
    print(f"  Failed: {stats['failed']}")

    print("\n[Step 2] Processing BioProjects...")

    valid_ids = cncb_ids + ncbi_ids

    results = parallel_process_bioprojects(
        valid_ids,
        output_path,
        email,
        api_key,
        max_workers,
        state_manager
    )

    print("\n[Step 3] Generating status report...")
    status_df = generate_status_report(bioproject_ids, state_manager, output_path)

    print("\n[Step 4] Final merge...")
    final_df = merge_all_results(results, output_path)

    print("\n" + "="*70)
    print("PIPELINE V4.2 COMPLETE")
    print("="*70)
    print(f"Total BioProjects: {len(bioproject_ids)}")
    print(f"  - With valid Run data: {len(results)}")
    print(f"  - No data/No Run info: {len(bioproject_ids) - len(results)}")
    print(f"\nOutput files:")
    print(f"  - status.tsv: {len(status_df)} records")
    print(f"  - all_metadata_merged.csv: {len(final_df)} records")
    print(f"\nCheckpoints: {output_folder}/checkpoints/")
    print("="*70 + "\n")

    return {
        'bioproject_ids': bioproject_ids,
        'results': results,
        'final_df': final_df,
        'status_df': status_df
    }


# ============================================================================
# Command Line Interface
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='Unified Metadata Download Pipeline V4.2',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument('-o', '--output', required=True,
                       help='Output directory for metadata files')

    parser.add_argument('--keywords', action='store_true',
                       help='Enable keyword search mode')
    parser.add_argument('--field', nargs='+', default=None,
                       help='Search fields (required if --keywords set)')
    parser.add_argument('--organism', nargs='+', default=None,
                       help='Organism terms (required if --keywords set)')
    parser.add_argument('--opt', nargs='+', default=None,
                       help='Optional search terms')

    parser.add_argument('-i', '--input', default=None,
                       help='Input directory with BioProject IDs (required if --keywords not set)')

    parser.add_argument('-e', '--email', default=None,
                       help='Email for NCBI (auto-generated if not provided)')
    parser.add_argument('-k', '--api-key', default=None,
                       help='NCBI API key (optional)')
    parser.add_argument('-w', '--max-workers', type=int, default=None,
                       help='Max parallel workers')

    args = parser.parse_args()

    if args.email is None:
        args.email = generate_fake_email()
        print(f"Using auto-generated email: {args.email}")

    if args.keywords:
        if not args.field or not args.organism:
            print("ERROR: --keywords mode requires both --field and --organism")
            parser.print_help()
            sys.exit(1)

        print("\n=== KEYWORD SEARCH MODE ===")
        print(f"Fields: {args.field}")
        print(f"Organisms: {args.organism}")
        if args.opt:
            print(f"Optional: {args.opt}")
        print("===========================\n")

        try:
            print("[Step 1] Running keyword search...\n")
            downloader = BioProjectDownloader(args.email)
            search_results = downloader.search_and_download_batch(
                field=args.field,
                organism=args.organism,
                opt=args.opt,
                output_dir=Path(args.output),
                databases=["ncbi", "cncb"],
                delay=1.0
            )

            print("\n[Step 2] Combining search results...\n")
            combined_df = downloader.combine_batch_results(search_results)

            if combined_df.empty:
                print("ERROR: No BioProjects found")
                sys.exit(1)

            bioproject_input_folder = search_results['dirs']['results']
            print(f"\n[Step 3] Processing {len(combined_df)} BioProjects...\n")

            results = run_unified_pipeline_v4(
                str(bioproject_input_folder),
                args.output,
                args.email,
                args.api_key,
                args.max_workers
            )

            if results is None:
                sys.exit(1)

            print("\n✓ Keywords mode completed!")
            sys.exit(0)

        except KeyboardInterrupt:
            print("\n\nInterrupted by user")
            sys.exit(1)
        except Exception as e:
            print(f"\n\nERROR: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)

    else:
        if not args.input:
            print("ERROR: --input required when not using --keywords mode")
            parser.print_help()
            sys.exit(1)

        if not os.path.isdir(args.input):
            print(f"ERROR: Input directory not found: {args.input}")
            sys.exit(1)

        print("\n=== BIOPROJECT INPUT MODE ===")
        print(f"Input: {args.input}")
        print("==============================\n")

        try:
            results = run_unified_pipeline_v4(
                args.input,
                args.output,
                args.email,
                args.api_key,
                args.max_workers
            )

            if results is None:
                sys.exit(1)

            sys.exit(0)

        except KeyboardInterrupt:
            print("\n\nInterrupted by user")
            sys.exit(1)
        except Exception as e:
            print(f"\n\nERROR: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)


if __name__ == "__main__":
    main()
